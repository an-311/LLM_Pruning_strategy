{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJYqktgLyr5zto/d2POGQb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/an-311/LLM_Pruning_strategy/blob/main/Grp_Proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UkaST83ewvqJ",
        "outputId": "829facba-a60f-4bb3-b9ca-e353e69a7910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/357.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m357.5/357.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hğŸ“ Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "âœ“ Drive mounted successfully!\n",
            "\n",
            "Please provide the path to your JSONL file in Google Drive.\n",
            "Example paths:\n",
            "  - /content/drive/MyDrive/response_raw.jsonl\n",
            "  - /content/drive/MyDrive/datasets/response_raw.jsonl\n",
            "\n",
            "Enter the full path to your JSONL file: /content/drive/MyDrive/response_raw.jsonl\n",
            "âœ“ Found file: /content/drive/MyDrive/response_raw.jsonl (67.0 MB)\n",
            "\n",
            "ğŸ“Š Dataset contains 5000 samples\n",
            "\n",
            "ğŸ”‘ Choose your LLM provider:\n",
            "1 = OpenAI (recommended: gpt-4o-mini)\n",
            "2 = Anthropic (claude-3-5-sonnet-20241022)\n",
            "3 = Mock (for testing without API costs)\n",
            "Enter choice (1/2/3): 1\n",
            "Enter your OpenAI API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ“ Using provider: openai, model: gpt-4o-mini\n",
            "\n",
            "ğŸ“¥ Cloning GitHub repository...\n",
            "Cloning into 'LLM_Pruning_strategy'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 12 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (12/12), 8.13 KiB | 8.13 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "/content/LLM_Pruning_strategy\n",
            "âœ“ Repository cloned\n",
            "âœ“ Injected judges/OpenAIJudge\n",
            "\n",
            "ğŸš€ Starting LLM-as-a-judge scoring...\n",
            "Scoring: 100% 5000/5000 [4:46:52<00:00,  3.44s/it]\n",
            "\n",
            "Wrote 5000 scored rows to scored.jsonl\n",
            "Wrote top-1k to top1k.jsonl\n",
            "\n",
            "âœ… Processing complete!\n",
            "\n",
            "======================================================================\n",
            "ğŸ“Š RESULTS PREVIEW\n",
            "======================================================================\n",
            "\n",
            "ğŸ† Top 5 samples by aggregate score:\n",
            "\n",
            "Rank 1: ID 3611, Aggregate 10.0\n",
            "  Helpfulness 10, Factuality 10, Completeness 10, Adherence 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'question'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3974651822.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         print(f\"  Helpfulness {item['helpfulness']}, Factuality {item['factuality']}, \"\n\u001b[1;32m    171\u001b[0m               f\"Completeness {item['completeness']}, Adherence {item['adherence']}\")\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Question: {item['question'][:100]}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Rationale: {item['rationale']}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'question'"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q --upgrade openai anthropic tqdm\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "import os, json, statistics\n",
        "\n",
        "print(\" Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"âœ“ Drive mounted successfully!\\n\")\n",
        "\n",
        "\n",
        "print(\"Please provide the path to your JSONL file in Google Drive.\")\n",
        "print(\"Example paths:\")\n",
        "print(\"  - /content/drive/MyDrive/response_raw.jsonl\")\n",
        "print(\"  - /content/drive/MyDrive/datasets/response_raw.jsonl\\n\")\n",
        "\n",
        "input_file = input(\"Enter the full path to your JSONL file: \").strip()\n",
        "\n",
        "\n",
        "if os.path.exists(input_file):\n",
        "    file_size_mb = os.path.getsize(input_file) / (1024 * 1024)\n",
        "    print(f\"âœ“ Found file: {input_file} ({file_size_mb:.1f} MB)\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"File not found: {input_file}\")\n",
        "\n",
        "\n",
        "sample_count = sum(1 for line in open(input_file) if line.strip())\n",
        "print(f\"\\n Dataset contains {sample_count} samples\")\n",
        "\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "print(\"\\n Choose your LLM provider:\")\n",
        "print(\"1 = OpenAI (recommended: gpt-4o-mini)\")\n",
        "print(\"2 = Anthropic (claude-3-5-sonnet-20241022)\")\n",
        "print(\"3 = Mock (for testing without API costs)\")\n",
        "\n",
        "provider_choice = input(\"Enter choice (1/2/3): \").strip()\n",
        "\n",
        "if provider_choice == \"1\":\n",
        "    provider = \"openai\"\n",
        "    model = \"gpt-4o-mini\"\n",
        "    api_key = getpass(\"Enter your OpenAI API key: \")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "elif provider_choice == \"2\":\n",
        "    provider = \"anthropic\"\n",
        "    model = \"claude-3-5-sonnet-20241022\"\n",
        "    api_key = getpass(\"Enter your Anthropic API key: \")\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = api_key\n",
        "elif provider_choice == \"3\":\n",
        "    provider = \"mock\"\n",
        "    model = \"mock\"\n",
        "else:\n",
        "    raise ValueError(\"Invalid choice\")\n",
        "\n",
        "print(f\" Using provider: {provider}, model: {model}\")\n",
        "\n",
        "!rm -rf LLM_Pruning_strategy\n",
        "!git clone https://github.com/an-311/LLM_Pruning_strategy/\n",
        "%cd LLM_Pruning_strategy\n",
        "\n",
        "!mkdir -p judges\n",
        "with open(\"judges/__init__.py\", \"w\") as f: f.write(\"# package init\\n\")\n",
        "\n",
        "judge_code = r'''\n",
        "import os, json, re, time, random\n",
        "from typing import Dict, Any\n",
        "from openai import OpenAI\n",
        "from openai import RateLimitError, APIStatusError\n",
        "\n",
        "class OpenAIJudge:\n",
        "    \"\"\"\n",
        "    Minimal judge that uses the OpenAI SDK Responses API.\n",
        "    Exposes .score(question, answer) -> Dict[str, Any].\n",
        "    \"\"\"\n",
        "    def __init__(self, model: str):\n",
        "        self.model = model\n",
        "        api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise RuntimeError(\"OPENAI_API_KEY not set\")\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "\n",
        "    def _call_with_retries(self, prompt: str, max_retries: int = 6) -> str:\n",
        "        backoff = 1.0\n",
        "        for _ in range(max_retries):\n",
        "            try:\n",
        "                resp = self.client.responses.create(model=self.model, input=prompt)\n",
        "                return getattr(resp, \"output_text\", \"\") or \"\"\n",
        "            except RateLimitError:\n",
        "                time.sleep(backoff + random.uniform(0,0.5))\n",
        "                backoff = min(backoff*2, 30)\n",
        "            except APIStatusError as e:\n",
        "                if 500 <= e.status_code < 600:\n",
        "                    time.sleep(backoff + random.uniform(0,0.5))\n",
        "                    backoff = min(backoff*2, 30)\n",
        "                else:\n",
        "                    raise\n",
        "        return \"\"\n",
        "\n",
        "    def score(self, question: str, answer: str) -> Dict[str, Any]:\n",
        "        prompt = f\"\"\"\n",
        "You are an impartial LLM judge.\n",
        "Score the candidate answer on 0â€“10 for:\n",
        "- helpfulness\n",
        "- factuality\n",
        "- completeness\n",
        "- adherence (to the user's ask)\n",
        "\n",
        "Return ONLY a JSON object with keys:\n",
        "helpfulness, factuality, completeness, adherence, rationale\n",
        "\n",
        "# Question\n",
        "{question}\n",
        "\n",
        "# Candidate Answer\n",
        "{answer}\n",
        "\"\"\"\n",
        "        text = self._call_with_retries(prompt)\n",
        "\n",
        "        m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n",
        "        if not m:\n",
        "            return {\n",
        "                \"helpfulness\": 0, \"factuality\": 0, \"completeness\": 0, \"adherence\": 0,\n",
        "                \"rationale\": \"Judge returned unparsable output.\"\n",
        "            }\n",
        "        try:\n",
        "            payload = json.loads(m.group(0))\n",
        "        except Exception:\n",
        "            payload = {\n",
        "                \"helpfulness\": 0, \"factuality\": 0, \"completeness\": 0, \"adherence\": 0,\n",
        "                \"rationale\": \"Judge returned invalid JSON.\"\n",
        "            }\n",
        "        for k in (\"helpfulness\",\"factuality\",\"completeness\",\"adherence\"):\n",
        "            try:\n",
        "                payload[k] = max(0, min(10, float(payload.get(k, 0))))\n",
        "            except Exception:\n",
        "                payload[k] = 0.0\n",
        "        if \"rationale\" not in payload:\n",
        "            payload[\"rationale\"] = \"No rationale.\"\n",
        "        return payload\n",
        "'''\n",
        "with open(\"judges/openai_judge.py\", \"w\") as f: f.write(judge_code)\n",
        "print(\"Injected judges/OpenAIJudge\")\n",
        "\n",
        "\n",
        "!python Prune.py \\\n",
        "    --in \"{input_file}\" \\\n",
        "    --out_scored scored.jsonl \\\n",
        "    --out_top top1k.jsonl \\\n",
        "    --provider {provider} \\\n",
        "    --model {model} \\\n",
        "    --seed 42 \\\n",
        "    --max_concurrency 1\n",
        "\n",
        "print(\"\\n Processing complete!\")\n"
      ]
    }
  ]
}