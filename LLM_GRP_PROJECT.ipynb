{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh+p8Y4tjZfl6r6fD1L1ZZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/an-311/LLM_Pruning_strategy/blob/main/LLM_GRP_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fkZgvBX8xtA"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade openai anthropic tqdm\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "import os, json, statistics\n",
        "\n",
        "print(\" Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"✓ Drive mounted successfully!\\n\")\n",
        "\n",
        "\n",
        "print(\"Please provide the path to your JSONL file in Google Drive.\")\n",
        "print(\"Example paths:\")\n",
        "print(\"  - /content/drive/MyDrive/response_raw.jsonl\")\n",
        "print(\"  - /content/drive/MyDrive/datasets/response_raw.jsonl\\n\")\n",
        "\n",
        "input_file = input(\"Enter the full path to your JSONL file: \").strip()\n",
        "\n",
        "\n",
        "if os.path.exists(input_file):\n",
        "    file_size_mb = os.path.getsize(input_file) / (1024 * 1024)\n",
        "    print(f\"✓ Found file: {input_file} ({file_size_mb:.1f} MB)\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"File not found: {input_file}\")\n",
        "\n",
        "\n",
        "sample_count = sum(1 for line in open(input_file) if line.strip())\n",
        "print(f\"\\n Dataset contains {sample_count} samples\")\n",
        "\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "print(\"\\n Choose your LLM provider:\")\n",
        "print(\"1 = OpenAI (recommended: gpt-4o-mini)\")\n",
        "print(\"2 = Anthropic (claude-3-5-sonnet-20241022)\")\n",
        "print(\"3 = Mock (for testing without API costs)\")\n",
        "\n",
        "provider_choice = input(\"Enter choice (1/2/3): \").strip()\n",
        "\n",
        "if provider_choice == \"1\":\n",
        "    provider = \"openai\"\n",
        "    model = \"gpt-4o-mini\"\n",
        "    api_key = getpass(\"Enter your OpenAI API key: \")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "elif provider_choice == \"2\":\n",
        "    provider = \"anthropic\"\n",
        "    model = \"claude-3-5-sonnet-20241022\"\n",
        "    api_key = getpass(\"Enter your Anthropic API key: \")\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = api_key\n",
        "elif provider_choice == \"3\":\n",
        "    provider = \"mock\"\n",
        "    model = \"mock\"\n",
        "else:\n",
        "    raise ValueError(\"Invalid choice\")\n",
        "\n",
        "print(f\" Using provider: {provider}, model: {model}\")\n",
        "\n",
        "!rm -rf LLM_Pruning_strategy\n",
        "!git clone https://github.com/an-311/LLM_Pruning_strategy/\n",
        "%cd LLM_Pruning_strategy\n",
        "\n",
        "!mkdir -p judges\n",
        "with open(\"judges/__init__.py\", \"w\") as f: f.write(\"# package init\\n\")\n",
        "\n",
        "judge_code = r'''\n",
        "import os, json, re, time, random\n",
        "from typing import Dict, Any\n",
        "from openai import OpenAI\n",
        "from openai import RateLimitError, APIStatusError\n",
        "\n",
        "class OpenAIJudge:\n",
        "    \"\"\"\n",
        "    Minimal judge that uses the OpenAI SDK Responses API.\n",
        "    Exposes .score(question, answer) -> Dict[str, Any].\n",
        "    \"\"\"\n",
        "    def __init__(self, model: str):\n",
        "        self.model = model\n",
        "        api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise RuntimeError(\"OPENAI_API_KEY not set\")\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "\n",
        "    def _call_with_retries(self, prompt: str, max_retries: int = 6) -> str:\n",
        "        backoff = 1.0\n",
        "        for _ in range(max_retries):\n",
        "            try:\n",
        "                resp = self.client.responses.create(model=self.model, input=prompt)\n",
        "                return getattr(resp, \"output_text\", \"\") or \"\"\n",
        "            except RateLimitError:\n",
        "                time.sleep(backoff + random.uniform(0,0.5))\n",
        "                backoff = min(backoff*2, 30)\n",
        "            except APIStatusError as e:\n",
        "                if 500 <= e.status_code < 600:\n",
        "                    time.sleep(backoff + random.uniform(0,0.5))\n",
        "                    backoff = min(backoff*2, 30)\n",
        "                else:\n",
        "                    raise\n",
        "        return \"\"\n",
        "\n",
        "    def score(self, question: str, answer: str) -> Dict[str, Any]:\n",
        "        prompt = f\"\"\"\n",
        "You are an impartial LLM judge.\n",
        "Score the candidate answer on 0–10 for:\n",
        "- helpfulness\n",
        "- factuality\n",
        "- completeness\n",
        "- adherence (to the user's ask)\n",
        "\n",
        "Return ONLY a JSON object with keys:\n",
        "helpfulness, factuality, completeness, adherence, rationale\n",
        "\n",
        "# Question\n",
        "{question}\n",
        "\n",
        "# Candidate Answer\n",
        "{answer}\n",
        "\"\"\"\n",
        "        text = self._call_with_retries(prompt)\n",
        "\n",
        "        m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n",
        "        if not m:\n",
        "            return {\n",
        "                \"helpfulness\": 0, \"factuality\": 0, \"completeness\": 0, \"adherence\": 0,\n",
        "                \"rationale\": \"Judge returned unparsable output.\"\n",
        "            }\n",
        "        try:\n",
        "            payload = json.loads(m.group(0))\n",
        "        except Exception:\n",
        "            payload = {\n",
        "                \"helpfulness\": 0, \"factuality\": 0, \"completeness\": 0, \"adherence\": 0,\n",
        "                \"rationale\": \"Judge returned invalid JSON.\"\n",
        "            }\n",
        "        for k in (\"helpfulness\",\"factuality\",\"completeness\",\"adherence\"):\n",
        "            try:\n",
        "                payload[k] = max(0, min(10, float(payload.get(k, 0))))\n",
        "            except Exception:\n",
        "                payload[k] = 0.0\n",
        "        if \"rationale\" not in payload:\n",
        "            payload[\"rationale\"] = \"No rationale.\"\n",
        "        return payload\n",
        "'''\n",
        "with open(\"judges/openai_judge.py\", \"w\") as f: f.write(judge_code)\n",
        "print(\"Injected judges/OpenAIJudge\")\n",
        "\n",
        "\n",
        "!python Prune.py \\\n",
        "    --in \"{input_file}\" \\\n",
        "    --out_scored scored.jsonl \\\n",
        "    --out_top top1k.jsonl \\\n",
        "    --provider {provider} \\\n",
        "    --model {model} \\\n",
        "    --seed 42 \\\n",
        "    --max_concurrency 1\n",
        "\n",
        "print(\"\\n Processing complete!\")"
      ]
    }
  ]
}